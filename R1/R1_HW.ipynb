{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOPXxjexDMDZ"
   },
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hvn98KdEE9_"
   },
   "source": [
    "將`關鍵字`比對換成`向量相似度`比對。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV4-rfIDDz7D"
   },
   "source": [
    "> 請將目前使用關鍵字比對的 route_by_query，改為使用向量相似度進行分類，並設一個合理的相似度門檻，根據檢索結果的分數判斷是否走 RAG 流程。  \n",
    "例如用向量相似度及自訂 threshold 決定要不要分到 retriever。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2jMeAOuJ2OM"
   },
   "source": [
    "> Hint：similarity_search_with_score(...)  \n",
    "可參考去年的讀書會 R4：向量資料庫的基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDN-jWJhNNNr"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain langgraph transformers bitsandbytes langchain-huggingface langchain-community chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c5812b8db15f47bcaf6e7cfe37013186",
      "545f54fbf88343d684844fccee7ef029",
      "57db00cd3fb5497f8731bc276753ae6d",
      "c0b20505cb5a4cb0b96b49ce417dc105",
      "41f293835a884f23b153d370093e3de3",
      "41d456c9b0574bca82357329702e4950",
      "eebfbc41e904438f82f8b52ac6fe5245",
      "04ec260dcd15476081275c15573bdc28",
      "4ea87d9455c645689f53af8202827ae3",
      "8a2172a8226a4f6e9f98d3fcd5038b2a",
      "a363bdb554334c5882971c40b7c10475"
     ]
    },
    "id": "Zt9F7rMn70O7",
    "outputId": "3678f0d7-6443-44fb-c7f1-762d73850ef6"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# 使用 4-bit 量化模型\n",
    "model_id = \"MediaTek-Research/Breeze-7B-Instruct-v1_0\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "# 載入 tokenizer 與 4-bit 模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RCs46fd8gyW",
    "outputId": "b502a013-3a47-49d9-c98e-8d978f1ad147"
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "docs_text = \"\"\"\n",
    "火影代數\t姓名\t師傅\t徒弟\n",
    "初代\t千手柱間\t無明確記載\t猿飛日斬、水戶門炎、轉寢小春\n",
    "二代\t千手扉間\t千手柱間（兄長）\t猿飛日斬、志村團藏、宇智波鏡等\n",
    "三代\t猿飛日斬\t千手柱間、千手扉間\t自來也、大蛇丸、千手綱手（傳說三忍）\n",
    "四代\t波風湊\t自來也\t旗木卡卡西、宇智波帶土、野原琳\n",
    "五代\t千手綱手\t猿飛日斬\t春野櫻、志乃等（主要為春野櫻）\n",
    "六代\t旗木卡卡西\t波風湊\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\n",
    "七代\t漩渦鳴人\t自來也、旗木卡卡西\t木葉丸等（主要為木葉丸）\n",
    "\"\"\"\n",
    "\n",
    "docs = [Document(page_content=txt.strip()) for txt in docs_text.strip().split(\"\\n\\n\")]\n",
    "\n",
    "# chromadb 預設使用的大型語言模型為 \"all-MiniLM-L6-v2\"，由於該大型語言模型不支持中文，所以將模型替換為 \"infgrad/stella-base-zh-v3-1792d\"，並對 embedding 進行量化\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"infgrad/stella-base-zh-v3-1792d\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "persist_path = \"document_store\"\n",
    "collection_name = \"naruto_collection\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_path,\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICNAAEmo_if9",
    "outputId": "78a75062-fc39-4ce9-b37c-032ce0a5c098"
   },
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    "    return_full_text=False # 僅返回生成的回應內容\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vePsgkta_nIG"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, List\n",
    "\n",
    "# 定義 LangGraph 的 State 結構\n",
    "class RAGState(TypedDict):\n",
    "    query: str\n",
    "    docs: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJ9ZrdVK_qm7"
   },
   "outputs": [],
   "source": [
    "def retrieve_node(state: RAGState) -> RAGState:\n",
    "    query = state[\"query\"]\n",
    "    # similarity_search 距離越小越相似\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    return {\"query\": query, \"docs\": docs, \"answer\": \"\"}\n",
    "\n",
    "def generate_node(state: RAGState) -> RAGState:\n",
    "    query, docs = state[\"query\"], state[\"docs\"]\n",
    "    context = \"\\n\".join([d.page_content for d in docs])\n",
    "    prompt = (\n",
    "        f\"你是一個知識型助手，請根據以下內容回答問題：\\n\\n\"\n",
    "        f\"內容：{context}\\n\\n\"\n",
    "        f\"問題：{query}\\n\\n回答：\"\n",
    "    )\n",
    "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
    "    return {\"query\": query, \"docs\": docs, \"answer\": output}\n",
    "\n",
    "def direct_generate_node(state: RAGState) -> RAGState:\n",
    "    query = state[\"query\"]\n",
    "    prompt = f\"請回答以下問題：{query}\\n\\n回答：\"\n",
    "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
    "    return {\"query\": query, \"docs\": [], \"answer\": output}\n",
    "\n",
    "# 定義 Route Node（決定走哪條路）\n",
    "def route_by_query(state, threshold=0.75):  # 可調整門檻\n",
    "    query = state[\"query\"]\n",
    "    result = vectorstore.similarity_search_with_score(query, k=1)\n",
    "    docs, distance = result[0]\n",
    "    cosine_sim= 1-(distance**2)/2\n",
    "    choice = \"naruto\" if cosine_sim >= threshold else \"general\"\n",
    "    print(f\"跑到 → {choice}（相似度 = {cosine_sim:.4f}）\")\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3_B9Kq0_wCZ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# 建立 LangGraph 流程圖\n",
    "graph_builder = StateGraph(RAGState)\n",
    "\n",
    "graph_builder.set_entry_point(\"condition\")\n",
    "graph_builder.add_node(\"condition\", RunnableLambda(lambda x: x))  # 進來就分流，不改變內容\n",
    "graph_builder.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
    "graph_builder.add_node(\"generator\", RunnableLambda(generate_node))\n",
    "graph_builder.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
    "\n",
    "# 設定條件分流\n",
    "graph_builder.add_conditional_edges(\n",
    "    source=\"condition\",\n",
    "    path=RunnableLambda(route_by_query),\n",
    "    path_map={\n",
    "        \"naruto\": \"retriever\",\n",
    "        \"general\": \"direct_generator\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 接下來的正常連接\n",
    "graph_builder.add_edge(\"retriever\", \"generator\")\n",
    "graph_builder.add_edge(\"generator\", END)\n",
    "graph_builder.add_edge(\"direct_generator\", END)\n",
    "\n",
    "# 編譯 Graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "DvPFuV_r_yEp",
    "outputId": "da8557ab-a817-41d7-c333-1a7889408f57"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wflAEri34xe5",
    "outputId": "0114bcdb-0bfa-4cfb-c0b6-67abe8d2c0e8"
   },
   "outputs": [],
   "source": [
    "print(\"開始對話吧（輸入 q 結束）\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"使用者: \")\n",
    "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
    "        print(\"掰啦！\")\n",
    "        break\n",
    "\n",
    "    init_state: RAGState = {\n",
    "        \"query\": user_input,\n",
    "        \"docs\": [],\n",
    "        \"answer\": \"\"\n",
    "    }\n",
    "\n",
    "    result = graph.invoke(init_state)\n",
    "    raw_output = result[\"answer\"]\n",
    "\n",
    "    answer_text = raw_output.split(\"回答：\")[-1].strip()\n",
    "    print(\"回答：\", answer_text)\n",
    "    print(\"===\" * 20, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8blDsDnDpbO"
   },
   "source": [
    "# advance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofkLZjzHENNT"
   },
   "source": [
    "改成能支援多輪問答（Multi-turn RAG），並能根據前面的query判斷問題。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2VXz7FxEONI"
   },
   "source": [
    "> 請將 RAGState 加入 history 欄位，並在生成回答時，將歷史對話與當前問題一起組成 prompt。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eSvoKFiKqP5"
   },
   "source": [
    "> Hint：\n",
    "```\n",
    "class MultiTurnRAGState(TypedDict):  \n",
    "    history: List[str]  \n",
    "    query: str  \n",
    "    docs: List[Document]  \n",
    "    answer: str\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2O2-u24YEmAY"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, List\n",
    "\n",
    "# 定義 LangGraph 的 State 結構\n",
    "# class RAGState(TypedDict):\n",
    "#     query: str\n",
    "#     docs: List[Document]\n",
    "#     answer: str\n",
    "class MultiTurnRAGState(TypedDict):\n",
    "    history: List[str]\n",
    "    query: str\n",
    "    docs: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd0iYUJ6FxOF"
   },
   "outputs": [],
   "source": [
    "def retrieve_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
    "    query = state[\"query\"]\n",
    "    # similarity_search 距離越小越相似\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    return {\"query\": query, \"docs\": docs, \"answer\": \"\"}\n",
    "\n",
    "def generate_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
    "    query, docs, history = state[\"query\"], state[\"docs\"], state[\"history\"]\n",
    "    context = \"\\n\".join([d.page_content for d in docs])\n",
    "    prompt = (\n",
    "        f\"你是一個知識型助手，請根據以下內容回答問題：\\n\\n\"\n",
    "        f\"內容：{context}\\n\\n\"\n",
    "        f\"歷史對話: {history}\"\n",
    "        f\"問題：{query}\\n\\n回答：\"\n",
    "    )\n",
    "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
    "    state[\"history\"].append(query)\n",
    "    return {\"query\": query, \"docs\": docs, \"answer\": output}\n",
    "\n",
    "def direct_generate_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
    "    query, history = state[\"query\"], state[\"history\"]\n",
    "    prompt = f\"請回答以下問題：{query}, 並參考歷史對話回答\\n\\n history:{history}\\n\\n回答：\"\n",
    "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
    "    state[\"history\"].append(query)\n",
    "    return {\"query\": query, \"docs\": [], \"answer\": output}\n",
    "\n",
    "# 定義 Route Node（決定走哪條路）\n",
    "def route_by_query(state, threshold=0.75):  # 可調整門檻\n",
    "    query = state[\"query\"]\n",
    "    result = vectorstore.similarity_search_with_score(query, k=1)\n",
    "    docs, distance = result[0]\n",
    "    cosine_sim= 1-(distance**2)/2\n",
    "    choice = \"naruto\" if cosine_sim >= threshold else \"general\"\n",
    "    print(f\"跑到 → {choice}（相似度 = {cosine_sim:.4f}）\")\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrPeu2o1GZsp"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# 建立 LangGraph 流程圖\n",
    "graph_builder = StateGraph(MultiTurnRAGState)\n",
    "\n",
    "graph_builder.set_entry_point(\"condition\")\n",
    "graph_builder.add_node(\"condition\", RunnableLambda(lambda x: x))  # 進來就分流，不改變內容\n",
    "graph_builder.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
    "graph_builder.add_node(\"generator\", RunnableLambda(generate_node))\n",
    "graph_builder.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
    "\n",
    "# 設定條件分流\n",
    "graph_builder.add_conditional_edges(\n",
    "    source=\"condition\",\n",
    "    path=RunnableLambda(route_by_query),\n",
    "    path_map={\n",
    "        \"naruto\": \"retriever\",\n",
    "        \"general\": \"direct_generator\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 接下來的正常連接\n",
    "graph_builder.add_edge(\"retriever\", \"generator\")\n",
    "graph_builder.add_edge(\"generator\", END)\n",
    "graph_builder.add_edge(\"direct_generator\", END)\n",
    "\n",
    "# 編譯 Graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-Jrna_evTCC",
    "outputId": "123a819a-e375-4d07-c6bf-ddb56e7837dd"
   },
   "outputs": [],
   "source": [
    "global_history: List[str] = []\n",
    "\n",
    "print(\"開始對話吧（輸入 q 結束）\")\n",
    "while True:\n",
    "    user_input = input(\"使用者: \")\n",
    "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
    "        print(\"掰啦！\")\n",
    "        break\n",
    "\n",
    "    state = {\"history\": global_history, \"query\": user_input}\n",
    "    result = graph.invoke(state)\n",
    "\n",
    "    answer = result[\"answer\"].split(\"回答：\")[-1].strip()\n",
    "    print(\"AI 助理:\", answer)\n",
    "    print(\"===\" * 60, \"\\n\")\n",
    "\n",
    "    global_history = result[\"history\"]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
